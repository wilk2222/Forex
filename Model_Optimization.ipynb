{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d72e09-8a8e-47c9-b037-3e71faed2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Neural Network packages from Deep Learning Directory\n",
    "from DeepLearning import Keras_CNN1D as CNN\n",
    "from DeepLearning import Keras_FFNN as FFNN\n",
    "from DeepLearning import Keras_LSTM as LSTM\n",
    "from DeepLearning import Keras_MLP as MLP\n",
    "from DeepLearning import ModularNN as mNN\n",
    "from DeepLearning import Keras_Perceptron as Perceptron\n",
    "from DeepLearning import Keras_RBF as RBF\n",
    "from DeepLearning import Keras_RNN as RNN \n",
    "from DeepLearning import Keras_SNN as SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055f0d3-e9ca-472d-a823-f0b06c8c1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Technical Application and Wavelet denoising functions\n",
    "from pre_processing import ta_features as ta\n",
    "from pre_processing import wavelet_fx as denoise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ea7e07c-fc4b-441f-a73f-8da3be604355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neural network libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc54a37-5602-481c-802b-a8dbc81f1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Graph libraries\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374123db-0add-4c1b-8ce0-7580125c7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import mglearn\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "def import_dataframes_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Imports all CSV files from a folder with separator='\\t' and returns a dictionary of dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping CSV file names to their corresponding dataframes.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            df_name = filename[:-4]  # remove .csv extension from filename\n",
    "            df = pd.read_csv(filepath, sep='\\t')\n",
    "            dataframes[df_name] = df\n",
    "    return dataframes\n",
    "\n",
    "data_daily = 'Historical_Data/Daily'\n",
    "data_M1 = 'Historical_Data/M1_2019_01_01__2021_01_01_0000'\n",
    "data_M5 = 'Historical_Data/M5_2019_01_01__2021_01_01_0000'\n",
    "data_M10 = 'Historical_Data/M10_2019_01_01__2021_01_01_0000'\n",
    "data_M15 = 'Historical_Data/M15_2019_06_15__2021_06_15_0000'\n",
    "data_M30 = 'Historical_Data/M30_2018_01_01__2022_01_01_0000'\n",
    "data_1H = 'Historical_Data/1H_2018_06_01__2022_06_01_0000'\n",
    "data_6H = 'Historical_Data/6H_2014_01_01__2022_01_01_0000'\n",
    "data_12H = 'Historical_Data/12H_2014_01_01__2022_01_01_0000'\n",
    "\n",
    "dataframes_daily = import_dataframes_from_folder(data_daily)\n",
    "dataframes_M1 = import_dataframes_from_folder(data_M1)\n",
    "dataframes_M5 = import_dataframes_from_folder(data_M5)\n",
    "dataframes_M10 = import_dataframes_from_folder(data_M10)\n",
    "dataframes_M15 = import_dataframes_from_folder(data_M15)\n",
    "dataframes_M30 = import_dataframes_from_folder(data_M30)\n",
    "dataframes_H1 = import_dataframes_from_folder(data_1H)\n",
    "dataframes_H6 = import_dataframes_from_folder(data_6H)\n",
    "dataframes_H12 = import_dataframes_from_folder(data_12H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d1ee3-a39c-48ca-93b6-a01b670fc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframes_daily['EURCHF_Daily']\n",
    "\n",
    "data = data.rename(columns = {'<DATE>':'Date','<OPEN>':'open', '<HIGH>':'high', '<LOW>':'low', '<CLOSE>':'close','<TICKVOL>':'tickvol','<VOL>':'volume','<SPREAD>':'spread'})\n",
    "\n",
    "data = ta.TA_APPLICATION(data)\n",
    "\n",
    "data\n",
    "\n",
    "data['price_change'] = data['close'].diff()\n",
    "volatility = np.std(data['price_change'])\n",
    "print(\"Market volatility: \", volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0487c5f-0317-404a-a55e-5ba4b10a6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataframes.keys())\n",
    "# data = dataframes['EURCHF_Daily']\n",
    "# data\n",
    "\n",
    "processed_data = dataframes_daily['EURCHF_Daily']\n",
    "\n",
    "processed_data = processed_data.rename(columns = {'<DATE>':'Date','<OPEN>':'Open', '<HIGH>':'High', '<LOW>':'Low', '<CLOSE>':'Close','<TICKVOL>':'tickvol','<VOL>':'volume','<SPREAD>':'spread'})\n",
    "\n",
    "processed_data['open_wave'],processed_data['high_wave'],processed_data['low_wave'],\\\n",
    "processed_data['close_wave']=denoise.denoising([processed_data['Open'],processed_data['High'],processed_data['Low'],processed_data['Close']])\n",
    "\n",
    "processed_data = processed_data[['open_wave', 'high_wave', 'low_wave', 'close_wave', 'tickvol', 'volume', 'spread']]\n",
    "processed_data = ta.TA_WAVE_APPLICATION(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df5ea8-84f7-42b7-a478-418bcb246de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up window for rolling data\n",
    "def window_data(data, window, feature_col_numbers, target_col_number):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window - 1):\n",
    "        features = data.iloc[i:(i + window), feature_col_numbers]\n",
    "        target = data.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Window settings for tuning\n",
    "window_size = 1\n",
    "# feature_columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]\n",
    "feature_columns = [0,1,2,3,18,22,21,22,23,26,30,31,32]\n",
    "# feature_columns = [0,1,2,3]\n",
    "target_column = 3\n",
    "X, y = window_data(processed_data, window_size, feature_columns, target_column)\n",
    "\n",
    "# Print target variable and selected features\n",
    "print(f\"Target variable: {processed_data.columns[target_column]}\")\n",
    "print(\"Selected features:\")\n",
    "for col in feature_columns:\n",
    "    print(processed_data.columns[col])\n",
    "    \n",
    "# Use 80% of the data for training and the remaineder for testing\n",
    "split = int(0.8 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]\n",
    "\n",
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train.reshape(-1, len(feature_columns)))\n",
    "X_train = scaler.transform(X_train.reshape(-1, len(feature_columns)))\n",
    "X_test = scaler.transform(X_test.reshape(-1, len(feature_columns)))\n",
    "scaler.fit(y_train)\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Fill NaN values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "print(np.isnan(X_train).any())\n",
    "print(np.isnan(X_test).any())\n",
    "print(np.isinf(X_train).any())\n",
    "print(np.isinf(X_test).any())\n",
    "\n",
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8eb7bc56-416c-4e00-b9e2-3f7bb4bc7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the model - > /DeepLearning/Keras_LSTM\n",
    "def build_model_lstm(hidden_layers=2, units=32, dropout_rate=0.1, output_units=1, optimizer='adam', loss='binary_crossentropy'):\n",
    "    \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(LSTM(units=units, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(hidden_layers - 1):\n",
    "            model.add(LSTM(units=units, return_sequences=True))\n",
    "            #model.add(BatchNormalization())\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(LSTM(units=output_units))\n",
    "        model.add(Dense(units=output_units))\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "# Define the function to create the model - > /DeepLearning/Keras_FFNN\n",
    "def build_model_ffnn(hidden_layers=2, units=32, dropout_rate=0.1, output_units=1, optimizer='adam', loss='binary_crossentropy'):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(Dense(units=units, input_shape=(X_train.shape[1], 1)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.hidden_layers - 1):\n",
    "            model.add(Dense(units=units))\n",
    "            #model.add(BatchNormalization())\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(units=output_units))\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        return model\n",
    "                      \n",
    "# Define the function to create the model - > /DeepLearning/Keras_MLP           \n",
    "def build_model_mlp(hidden_layers=2, units=32, dropout_rate=0.1, output_units=1, optimizer='adam', loss='binary_crossentropy'):\n",
    "                      \n",
    "        model = Sequential()\n",
    "\n",
    "        # Input layer\n",
    "        model.add(Dense(units=hidden_layers[0], input_shape=(X_train.shape[1], 1)))\n",
    "        #model.add(BatchNormalization())              \n",
    "        model.add(Activation(self.activation))\n",
    "\n",
    "        # Hidden layers\n",
    "        for layer_size in hidden_layers[1:]:\n",
    "            model.add(Dense(units=units))\n",
    "            #model.add(BatchNormalization())          \n",
    "            model.add(Activation(self.activation))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(units=output_units))\n",
    "        #model.add(BatchNormalization())              \n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Define the function to create the model - > /DeepLearning/Keras_Perceptron                                 \n",
    "def build_model_perceptron(hidden_layers=2, units=32, dropout_rate=0.1, output_units=1, optimizer='adam', loss='binary_crossentropy'):\n",
    "                      \n",
    "        model = Sequential()\n",
    "\n",
    "        # Input layer\n",
    "        model.add(Dense(units=output_units, input_shape=(X_train.shape[1], 1)))\n",
    "        #model.add(BatchNormalization())               \n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model\n",
    "                      \n",
    "# Define the function to create the model - > /DeepLearning/Keras_RNN                                                      \n",
    "def build_model_rnn(hidden_layers=2, units=32, dropout_rate=0.1, output_units=1, optimizer='adam', loss='binary_crossentropy'):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(SimpleRNN(units=units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(hidden_layers - 1):\n",
    "            model.add(SimpleRNN(units=self.units, return_sequences=True))\n",
    "            # model.add(BatchNormalization())\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(SimpleRNN(units=output_units))\n",
    "        model.add(Dense(units=output_units))\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        return model  \n",
    "\n",
    "# Define the function to create the model - > /DeepLearning/ModularNN                                                                       \n",
    "def build_model_ModularNN(hidden_layers=2, units=32, dropout_rate=0.1, output_units=1, optimizer='adam', loss='binary_crossentropy'):\n",
    "        inputs = Input(shape=(X_train.shape[1], 1))\n",
    "        x = inputs\n",
    "\n",
    "        for units, activation, dropout in hidden_layers:\n",
    "            x = Dense(units, activation=activation)(x)\n",
    "            if dropout > 0:\n",
    "                x = Dropout(dropout)(x)\n",
    "\n",
    "        outputs = Dense(output_shape, activation='softmax')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "334af11a-f1b6-450e-9696-cc89b56f7e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian\\AppData\\Local\\Temp\\ipykernel_5716\\4156450443.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_model_lstm, epochs=10, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=categorical_crossentropy, optimizer=rmsprop, units=2; total time=   7.2s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=categorical_crossentropy, optimizer=rmsprop, units=2; total time=   8.0s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=categorical_crossentropy, optimizer=rmsprop, units=2; total time=   7.1s\n",
      "[CV] END dropout_rate=0.30000000000000004, hidden_layers=2, loss=mse, optimizer=rmsprop, units=6; total time=   7.1s\n",
      "[CV] END dropout_rate=0.30000000000000004, hidden_layers=2, loss=mse, optimizer=rmsprop, units=6; total time=   7.2s\n",
      "[CV] END dropout_rate=0.30000000000000004, hidden_layers=2, loss=mse, optimizer=rmsprop, units=6; total time=   8.1s\n",
      "[CV] END dropout_rate=0.5, hidden_layers=1, loss=categorical_crossentropy, optimizer=rmsprop, units=8; total time=   5.0s\n",
      "[CV] END dropout_rate=0.5, hidden_layers=1, loss=categorical_crossentropy, optimizer=rmsprop, units=8; total time=   5.0s\n",
      "[CV] END dropout_rate=0.5, hidden_layers=1, loss=categorical_crossentropy, optimizer=rmsprop, units=8; total time=   5.0s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=categorical_crossentropy, optimizer=rmsprop, units=6; total time=   7.2s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=categorical_crossentropy, optimizer=rmsprop, units=6; total time=   7.2s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=categorical_crossentropy, optimizer=rmsprop, units=6; total time=   8.2s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=mse, optimizer=sgd, units=7; total time=   7.1s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=mse, optimizer=sgd, units=7; total time=   7.1s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=2, loss=mse, optimizer=sgd, units=7; total time=   7.1s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=1, loss=categorical_crossentropy, optimizer=rmsprop, units=1; total time=   4.9s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=1, loss=categorical_crossentropy, optimizer=rmsprop, units=1; total time=   6.0s\n",
      "[CV] END dropout_rate=0.4, hidden_layers=1, loss=categorical_crossentropy, optimizer=rmsprop, units=1; total time=   5.0s\n",
      "[CV] END dropout_rate=0.0, hidden_layers=2, loss=mse, optimizer=sgd, units=5; total time=   7.0s\n",
      "[CV] END dropout_rate=0.0, hidden_layers=2, loss=mse, optimizer=sgd, units=5; total time=   6.9s\n",
      "[CV] END dropout_rate=0.0, hidden_layers=2, loss=mse, optimizer=sgd, units=5; total time=   7.7s\n",
      "[CV] END dropout_rate=0.1, hidden_layers=3, loss=categorical_crossentropy, optimizer=adam, units=8; total time=  14.2s\n",
      "[CV] END dropout_rate=0.1, hidden_layers=3, loss=categorical_crossentropy, optimizer=adam, units=8; total time=  10.8s\n",
      "[CV] END dropout_rate=0.1, hidden_layers=3, loss=categorical_crossentropy, optimizer=adam, units=8; total time=  11.0s\n",
      "[CV] END dropout_rate=0.5, hidden_layers=1, loss=binary_crossentropy, optimizer=rmsprop, units=6; total time=   5.6s\n",
      "[CV] END dropout_rate=0.5, hidden_layers=1, loss=binary_crossentropy, optimizer=rmsprop, units=6; total time=   6.5s\n",
      "[CV] END dropout_rate=0.5, hidden_layers=1, loss=binary_crossentropy, optimizer=rmsprop, units=6; total time=   5.4s\n",
      "[CV] END dropout_rate=0.1, hidden_layers=2, loss=binary_crossentropy, optimizer=adam, units=7; total time=   8.3s\n",
      "[CV] END dropout_rate=0.1, hidden_layers=2, loss=binary_crossentropy, optimizer=adam, units=7; total time=   8.7s\n",
      "[CV] END dropout_rate=0.1, hidden_layers=2, loss=binary_crossentropy, optimizer=adam, units=7; total time=   8.4s\n",
      "{'units': 2, 'optimizer': 'rmsprop', 'loss': 'categorical_crossentropy', 'hidden_layers': 2, 'dropout_rate': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Create the KerasClassifier object\n",
    "model = KerasClassifier(build_fn=build_model_lstm, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid_classifier = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'hidden_layers': np.arange(1, 4),\n",
    "    'dropout_rate': np.linspace(0, 0.5, 6),\n",
    "    'units': np.arange(1, 10),\n",
    "    'loss': ['binary_crossentropy', 'mse', 'categorical_crossentropy']\n",
    "}\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid_classifier,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model using the RandomizedSearchCV object\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_regressor = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'hidden_layers': [1, 2],\n",
    "    'units': [32, 64],\n",
    "    'dropout_rate': [0.1, 0.2],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [10, 20],\n",
    "    'loss': ['binary_crossentropy', 'mse','categorical_crossentropy']\n",
    "}\n",
    "\n",
    "# # Create KerasRegressor object for Grid Search\n",
    "# model = KerasRegressor(build_fn=build_model_lstm, verbose=0)\n",
    "\n",
    "# # Create Grid Search object\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# # Fit Grid Search object to training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print best hyperparameters and R^2 score\n",
    "# print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "# print(\"R^2 score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85e87b-4b74-4a7d-9b05-0e4aedbf1cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
