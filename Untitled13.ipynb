{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd05047-f8a9-44c0-9b9e-e403520dfb5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/v8n2zbyd53q9vnyys64v0cjr0000gn/T/ipykernel_39793/3696583210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "# TensorFlow Docs Imports for Evaluation\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"forex_data.csv\", sep=\"\\t\")\n",
    "\n",
    "data = data.rename(columns = {',<DATE>':'Date','<OPEN>':'Open', '<HIGH>':'High', '<LOW>':'Low', '<CLOSE>':'Close','<TICKVOL>':'tickvol','<VOL>':'volume','<SPREAD>':'spread'})\n",
    "#data = data.loc[:, data.columns != 'date']\n",
    "\n",
    "#data['momentum'] = data['volume'] * (data['open'] - data['close'])\n",
    "data['avg_price'] = (data['low'] + data['high']) / 2\n",
    "data['range'] = data['high'] - data['low']\n",
    "data['OHLC_price'] = (data['open'] + data['high'] + data['low'] + data['close']) / 4\n",
    "data['oc_diff'] = data['open'] - data['close']\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dataset = data.copy().values.astype('float32')\n",
    "pca_features = data.columns.tolist()\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "data['pca'] = pca.fit_transform(dataset)\n",
    "\n",
    "data['open_wave'],data['high_wave'],data['low_wave'],\\\n",
    "data['close_wave']=denoising([data['Open'],data['High'],data['Low'],data['Close']])\n",
    "\n",
    "data.isna().sum()\n",
    "# Set up window for rolling data\n",
    "def window_data(data, window, feature_col_number, target_col_number):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window - 1):\n",
    "        features = data.iloc[i:(i + window), feature_col_number]\n",
    "        target = data.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "# Window settings for tuning\n",
    "window_size = 1\n",
    "feature_column = 3\n",
    "target_column = 3\n",
    "X, y = window_data(data, window_size, feature_column, target_column)\n",
    "# Use 70% of the data for training and the remaineder for testing\n",
    "split = int(0.8 * len(X))\n",
    "\n",
    "\"\"\"\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]\n",
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "scaler.fit(y)\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "\"\"\"\n",
    "\n",
    "def normalize_function(data):\n",
    "    min = np.amin(data, axis=0)\n",
    "    max = np.amax(data, axis=0)\n",
    "    return (data-min)/(max-min)\n",
    "\n",
    "X_train = normalize_function(X[: split])\n",
    "X_test = normalize_function(X[split:])\n",
    "y_train = normalize_function(y[: split])\n",
    "y_test = normalize_function(y[split:])\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "print (f\"X_train sample values:\\n{X_train[:3]} \\n\")\n",
    "print (f\"X_test sample values:\\n{X_test[:3]}\")\n",
    "\n",
    "# Build the LSTM model \n",
    "model = Sequential()\n",
    "\n",
    "number_units = 40\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 2\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 3\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error'\n",
    ")\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs = 250, shuffle = False, batch_size = 100, verbose = 1)\n",
    "\n",
    "# Epochs have a little effect, whereas batch_size significantly can increase performance of price results\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "plot_model(model, show_shapes=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698cc71-a3cd-421a-8df1-cf1f138d27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.inferno\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.title('Pearson correlation of features', y=1.05, size=15)\n",
    "sns.heatmap(data.corr(), linewidths=0.1, vmax=1.0, square=True, cmap-colormap, linecolor='white',\n",
    "            annot=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr[corr.index == 'close'], linewidths=0.1, vmax=1.0, square=True, cmap=colormap,\n",
    "            linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cba891-f20e-4b5a-ad65-262ec6fefb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(n_estimators = 100)\n",
    "forest = forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bd28f-2571-4b85-83d0-b504ff6a73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([forest.feature_importances_ for forest in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "column_list = data.columns.tolist()\n",
    "print(\"Feature Ranking:\")\n",
    "for f in range(X.shape[1]-1):\n",
    "    print(\"%d. %s %d (%f)\" % (f, column_list[indices[f]], indices[f], importances[indices[f]]))\n",
    "    \n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color='salmon', terr=std[indices], align='center')\n",
    "plt.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
